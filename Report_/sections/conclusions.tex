\section{Conclusions}

This paper presents a reinforcement learning approach to solving Blackjack, with considerable attention paid to the games probabilistic nature. Watkin's Q-Learning was employed with a reward relative to the square of the card, for situations with constant and changing probabilities. 

The training processes converged after 

Optimal policies were created which, with no usable ace, stopped hitting at 15 and, with a usable ace, stopped hitting at 18. These values contrast a statisticians optimum. 
