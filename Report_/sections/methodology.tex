\section{Methodology}

In training an agent to play Blackjack, an iterative Q-Learning approach was taken. Watkins' Q-Learning aims to learn the optimal 'q-value' for given state-action pairs in an environment, i.e., the respective value of making a certain move in a certain environmental state. This approach was chosen because it is a model-free, value-based, off-policy algorithm. Model free means that the agent does not build an understanding of its environment's functionality, value based means that the algorithm estimates the value of each state-action pair, and off-policy means that the optimal policy need not be followed. Q-learning solves problems modelled as MDP, meaning the probabilistic nature of Blackjack was realized by the model. The Bellman's equation for Q-Learning is
\begin{multline} \label{eq:Bellman}
    Q_{new}(s,a)=Q_{old}(s,a)+\\\alpha(\underbrace{R(s,a)+\gamma MaxQ(s',a')-Q_{old}(s,a)}_{\mathclap{\text{Temporal Difference}}})\text{.}
\end{multline}
Where \( s, a \) are the current state and action,  \( s', a' \) are the next state and action, \( \alpha \) is the learning rate, \( \gamma \) is the discount factor, \( R(s,a) \) is the reward received after taking action \( a \) in state \( s \), and \( Q(s,a) \) is the q-value for the state-action pair. The highlighted temporal difference is used to update an estimate based on other estimates, without waiting for a final outcome (known as bootstrapping) \cite{10.5555/3312046}. The learning rate \(\alpha\) decayed throughout training with the aim of reducing probability induced learning oscillations. It took the equation, 
\begin{equation} \label{eq:learning rate}
    \alpha = \alpha_{min} + (\alpha_{max} - \alpha_{min})e^{-\frac{1}{Dt}}
\end{equation}
For respective values for the minimum and maximum learning rate, decay rate \(D\), and number of iterations over the training process \(t\). The reward \(R(s,a)\), passed to the agent to drive learning, was defined by
\begin{equation} \label{reward}
    R(s,a) = S_{new}^2 - (S_{old}^2 - S_{new}^2)\delta_{Ace}
\end{equation}
Where \(\delta_{Ace}=1\) when an ace drops in value from 11 to 1, and \(\delta_{Ace}=0\) otherwise. This is used to proportionally penalize the agent for using an ace since it is less advantageous to hold an 1-valued ace than a 11-valued ace.

\medskip
Q-tables encapsulate the state-action pairs, entirely defining all possible moves in the system with their respective Q-value. Each action taken recalculates respective state-action Q-values using equation \ref{eq:Bellman}. This converges towards a terminal value which, for probabilistic situations like this, is the expected sum of future rewards. From this, the optimal policy was determined by selecting the action with the highest q-value for each state. 

% \begin{equation} \label{eq:optimal policy}
%     q_*(s,a) = \max_{\pi}q_{\pi}(s,a)\text{.}
% \end{equation}

Since an agent follows the optimal policy where possible, exploiting  and not exploring new information, an Epsilon-greedy algorithm was employed. This intends to explore less taken state-action avenues \(X\%\) of the time. 

\subsection{Infinite}

In the 'infinite' situation, the probability of each card being drawn was equal, meaning that the expected value of the next drawn card remained constant, so retaining prior knowledge of cards drawn posed no advantage to the agent. Therefore, the environment was composed of two state variables - current score and usable ace - and one action variable - hit/stick.

\subsection{Finite}

In the 'finite' situation, cards were drawn from a pile of finite number, meaning that the probability of drawing respective cards changed as the game progressed. This introduced a challenge that would ideally have been addressed by providing the agent with all previously dealt cards, where the agent would have learned to predict the probabilities of new cards and adjust its play strategy accordingly. However, incorporating all previous cards into the agent's calculations comes at a significant computational cost. 

\medskip
To balance accuracy with computational efficiency, the probability of losing was integrated into the Q-table (a concept from reinforcement learning where Q-values represent the expected utility of taking an action). This approach was chosen over, say, card counting because it provided a more accurate representation of the environment while maintaining relatively low complexity. Since these probabilities could have been deduced from previously dealt cards, providing the agent with this essentially skipped the step of learning probabilistic expectations. 

\medskip
To implement this continuous probability into the Q-table of finite dimension size, the probabilities were discretized, i.e., continuous probabilities were separated into bins of size 10\%. Therefore, the environment was composed of three variables - current score, usable ace, and probability of losing - and one action variable - hit/stick. 